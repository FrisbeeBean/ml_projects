{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## RL Agents using Q-Learning and Vanilla Policy Gradient\n",
        "\n",
        "I am Rishabh Jain and this is my task of making agents using q-learning and vpg"
      ],
      "metadata": {
        "id": "96VkjNVgcV9E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### STEP 1: Importing Libraries"
      ],
      "metadata": {
        "id": "itZeQMo-cvf6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import random\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.animation as animation\n",
        "import seaborn as sns\n",
        "from torch.distributions import Categorical"
      ],
      "metadata": {
        "id": "St5ekGl0c0cD"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### STEP 2: Making the Environment"
      ],
      "metadata": {
        "id": "u2JlhndEdfZ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "env = gym.make(\"FrozenLake-v1\", map_name=\"8x8\", is_slippery=True)\n",
        "state_size = env.observation_space.n\n",
        "action_size = env.action_space.n\n",
        "np.random.seed(1)\n",
        "random.seed(1)\n",
        "torch.manual_seed(1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TRzbR1xmdjDN",
        "outputId": "e62d7654-765e-433e-fc02-4765b274d678"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7cdd2bf89690>"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### STEP 3: The code for the Q-Learning"
      ],
      "metadata": {
        "id": "mDglRV1Xe0XT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "episodes = 20000\n",
        "gamma = 0.99\n",
        "alpha = 0.1\n",
        "epsilon = 1.0\n",
        "epsilon_decay = 0.9999999999999999\n",
        "min_epsilon = 0.3\n",
        "\n",
        "q_table = np.zeros((state_size, action_size))\n",
        "q_rewards = []\n",
        "success_count = 0\n",
        "\n",
        "for ep in range(episodes):\n",
        "    state, _ = env.reset(seed=1)\n",
        "    done = False\n",
        "    total_reward = 0\n",
        "    while not done:\n",
        "        if np.random.rand() < epsilon:\n",
        "            action = env.action_space.sample()\n",
        "        else:\n",
        "            action = np.argmax(q_table[state])\n",
        "\n",
        "        next_state, reward, terminated, truncated, _ = env.step(action)\n",
        "        done = terminated or truncated\n",
        "\n",
        "        q_table[state, action] += alpha * (\n",
        "            reward + gamma * np.max(q_table[next_state]) - q_table[state, action]\n",
        "        )\n",
        "\n",
        "        state = next_state\n",
        "        total_reward += reward\n",
        "\n",
        "    q_rewards.append(total_reward)\n",
        "    if total_reward > 0:\n",
        "        success_count += 1\n",
        "\n",
        "    epsilon = max(min_epsilon, epsilon * epsilon_decay)\n",
        "\n",
        "print(f\"Total Successes: {success_count}/{episodes}\")\n",
        "print(f\"Success Rate: {success_count / episodes * 100:.2f}%\")\n",
        "print(q_table)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tUmICu4sfRCv",
        "outputId": "0976f813-1edb-4890-b00c-e89aca84c921"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Successes: 26/20000\n",
            "Success Rate: 0.13%\n",
            "[[1.42862053e-01 1.44274683e-01 1.43726643e-01 1.44647893e-01]\n",
            " [1.45033946e-01 1.46166970e-01 1.46875454e-01 1.47628717e-01]\n",
            " [1.50074175e-01 1.52192201e-01 1.52956074e-01 1.54190606e-01]\n",
            " [1.56587036e-01 1.58172265e-01 1.61007725e-01 1.62909171e-01]\n",
            " [1.62817476e-01 1.70298246e-01 1.67540633e-01 1.68634077e-01]\n",
            " [1.72427443e-01 1.74145581e-01 1.79251398e-01 1.74217393e-01]\n",
            " [1.83118206e-01 1.81702795e-01 1.87284189e-01 1.83163299e-01]\n",
            " [1.89339980e-01 1.88193543e-01 1.90360698e-01 1.87443385e-01]\n",
            " [1.41735407e-01 1.43090817e-01 1.42392069e-01 1.43572626e-01]\n",
            " [1.43516447e-01 1.44203394e-01 1.45772390e-01 1.46668118e-01]\n",
            " [1.43982317e-01 1.45397485e-01 1.45497234e-01 1.51934309e-01]\n",
            " [1.27041616e-01 1.13752162e-01 1.15369192e-01 1.55887948e-01]\n",
            " [1.49075593e-01 1.50392409e-01 1.56563471e-01 1.63926328e-01]\n",
            " [1.64343684e-01 1.69198242e-01 1.72710438e-01 1.73117598e-01]\n",
            " [1.84000050e-01 1.88680511e-01 1.89906775e-01 1.84581976e-01]\n",
            " [1.92769015e-01 1.95901821e-01 1.92381604e-01 1.89064906e-01]\n",
            " [1.38956269e-01 1.37591595e-01 1.36280820e-01 1.41247682e-01]\n",
            " [1.34913406e-01 1.33264676e-01 1.35254341e-01 1.40807877e-01]\n",
            " [1.36142186e-01 9.19032409e-02 8.83469892e-02 1.20570364e-01]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
            " [1.26926005e-01 8.71897038e-02 1.22998605e-01 9.47693373e-02]\n",
            " [9.85646011e-02 1.28296173e-01 1.32979411e-01 1.66812375e-01]\n",
            " [1.83251161e-01 1.85862736e-01 1.93187896e-01 1.75664959e-01]\n",
            " [2.03745012e-01 2.06045508e-01 2.03935963e-01 1.97323368e-01]\n",
            " [1.25047563e-01 1.26214471e-01 1.25493041e-01 1.30834712e-01]\n",
            " [1.22431541e-01 1.14180917e-01 1.19957470e-01 1.24841333e-01]\n",
            " [1.08867612e-01 8.94097475e-02 8.80790824e-02 1.01078430e-01]\n",
            " [3.57149702e-02 5.19352330e-02 3.40188846e-02 4.92816751e-02]\n",
            " [7.05006923e-02 3.74335056e-02 4.94608000e-02 5.27918225e-02]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
            " [1.39442317e-01 1.05214611e-01 1.97172750e-01 1.13274949e-01]\n",
            " [2.21816842e-01 2.16153848e-01 2.17987400e-01 2.01316767e-01]\n",
            " [1.19362598e-01 1.01542590e-01 1.07879413e-01 1.15310490e-01]\n",
            " [5.85268813e-02 7.30929640e-02 6.05061190e-02 1.02390765e-01]\n",
            " [7.69722741e-02 3.78730048e-02 3.29481389e-02 7.22329570e-02]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
            " [4.56850961e-02 3.24569810e-02 4.51870908e-02 4.33883201e-02]\n",
            " [2.05504154e-02 7.69981207e-02 2.62320914e-02 7.44961375e-02]\n",
            " [8.85407764e-02 1.03638327e-01 1.43437338e-01 1.63358997e-01]\n",
            " [2.37305346e-01 2.17699957e-01 2.45733827e-01 2.20594439e-01]\n",
            " [8.30367682e-02 5.11425390e-02 5.58340215e-02 6.25700710e-02]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
            " [7.44875044e-05 2.64523462e-03 1.56399561e-03 5.09324192e-03]\n",
            " [1.74671766e-02 9.30932391e-03 2.73364625e-02 2.29170189e-02]\n",
            " [2.84544128e-02 6.15217047e-03 1.84451983e-02 2.32867465e-02]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
            " [1.84082627e-01 2.30238528e-01 2.99929271e-01 1.31705847e-01]\n",
            " [7.34772711e-02 4.20414799e-02 5.91020534e-02 4.79372488e-02]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
            " [4.20785550e-04 5.35860397e-04 7.89594831e-05 1.69683218e-05]\n",
            " [4.46615123e-04 1.92224569e-05 4.48943638e-04 2.78453008e-04]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
            " [1.93241343e-03 8.24752223e-06 7.09689239e-04 1.15915306e-03]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
            " [3.50117166e-01 3.97110130e-01 3.69842267e-01 1.44075892e-01]\n",
            " [5.79599093e-02 4.68255839e-02 5.30313217e-02 5.23501464e-02]\n",
            " [1.18154911e-02 2.38281044e-02 8.38700272e-03 2.54746098e-02]\n",
            " [5.95626557e-03 7.65566724e-03 1.04937897e-03 4.29617967e-03]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
            " [2.87499523e-04 9.91151098e-03 3.47268255e-04 1.00296224e-02]\n",
            " [9.89123959e-03 9.99000000e-02 0.00000000e+00 1.00000000e-01]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### STEP 4: The code of VPG"
      ],
      "metadata": {
        "id": "BOrQ8GBISbil"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PolicyNetwork(nn.Module):\n",
        "    def __init__(self, obs_dim, act_dim):\n",
        "        super(PolicyNetwork, self).__init__()\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(obs_dim, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, act_dim),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc(x)\n",
        "        return torch.softmax(x, dim=-1)\n",
        "\n",
        "def sample_action(policy, obs):\n",
        "    obs = torch.tensor(obs, dtype=torch.float32)\n",
        "    probs = policy(obs)\n",
        "    dist = torch.distributions.Categorical(probs)\n",
        "    action = dist.sample()\n",
        "    return action.item(), dist.log_prob(action)\n",
        "\n",
        "def compute_returns(rewards, gamma=0.99):\n",
        "    G = 0\n",
        "    returns = []\n",
        "    for r in reversed(rewards):\n",
        "        G = r + gamma * G\n",
        "        returns.insert(0, G)\n",
        "    returns = torch.tensor(returns, dtype=torch.float32)\n",
        "    return returns\n",
        "\n",
        "def train_vpg(policy, optimizer, episodes=20000):\n",
        "    vpg_rewards = []\n",
        "    vpg_success = 0\n",
        "    baseline = 0.0\n",
        "    for episode in range(episodes):\n",
        "        obs, _ = env.reset(seed=1)\n",
        "        obs = np.eye(state_size)[obs]\n",
        "        log_probs, rewards = [], []\n",
        "        done = False\n",
        "\n",
        "        while not done:\n",
        "            action, log_prob = sample_action(policy, obs)\n",
        "            next_obs, reward, terminated, truncated, _ = env.step(action)\n",
        "            done = terminated or truncated\n",
        "            log_probs.append(log_prob)\n",
        "            rewards.append(reward)\n",
        "            obs = np.eye(state_size)[next_obs]\n",
        "\n",
        "        returns = compute_returns(rewards)\n",
        "        baseline = 0.9 * baseline + 0.1 * returns.mean().item()\n",
        "        advantage = returns - baseline\n",
        "        loss = -torch.sum(torch.stack(log_probs) * advantage)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        episode_reward = sum(rewards)\n",
        "        vpg_rewards.append(sum(rewards))\n",
        "        if episode_reward > 0:\n",
        "            vpg_success += 1\n",
        "        if episode % 500 == 0:\n",
        "            print(f\"Episode {episode} — Reward: {sum(rewards)}\")\n",
        "\n",
        "    return vpg_rewards, vpg_success\n",
        "\n",
        "obs_dim = state_size\n",
        "act_dim = action_size\n",
        "policy = PolicyNetwork(obs_dim, act_dim)\n",
        "optimizer = optim.Adam(policy.parameters(), lr=1e-2)\n",
        "\n",
        "vpg_rewards, vpg_success = train_vpg(policy, optimizer)\n",
        "print(f\"Total Successes: {vpg_success}/{episodes}\")\n",
        "print(f\"Success Rate: {vpg_success / 20000 * 100:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n0NaebHGRBWl",
        "outputId": "114878c7-daad-4353-d5af-b2c32c5a60ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 0 — Reward: 0.0\n",
            "Episode 500 — Reward: 0.0\n",
            "Episode 1000 — Reward: 0.0\n",
            "Episode 1500 — Reward: 0.0\n",
            "Episode 2000 — Reward: 0.0\n",
            "Episode 2500 — Reward: 0.0\n",
            "Episode 3000 — Reward: 0.0\n",
            "Episode 3500 — Reward: 0.0\n",
            "Episode 4000 — Reward: 0.0\n",
            "Episode 4500 — Reward: 0.0\n",
            "Episode 5000 — Reward: 0.0\n",
            "Episode 5500 — Reward: 0.0\n",
            "Episode 6000 — Reward: 0.0\n",
            "Episode 6500 — Reward: 0.0\n",
            "Episode 7000 — Reward: 0.0\n",
            "Episode 7500 — Reward: 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### STEP 5: Visualization"
      ],
      "metadata": {
        "id": "mZpx6wD7wAFV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_q_table_heatmap(q_table, title=\"Q-Table Heatmap\"):\n",
        "    best_actions = np.argmax(q_table, axis=1).reshape((8, 8))\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.heatmap(best_actions, annot=True, cmap=\"YlGnBu\", cbar=False, linewidths=0.5)\n",
        "    plt.title(title)\n",
        "    plt.xlabel(\"Column\")\n",
        "    plt.ylabel(\"Row\")\n",
        "    plt.show()\n",
        "\n",
        "def plot_policy_probs(policy_net):\n",
        "    grid = np.zeros((8, 8))\n",
        "    action_grid = np.empty((8, 8), dtype=object)\n",
        "    for state in range(state_size):\n",
        "        one_hot = np.eye(state_size)[state]\n",
        "        one_hot_tensor = torch.tensor(one_hot, dtype=torch.float32)\n",
        "        with torch.no_grad():\n",
        "            probs = policy_net(one_hot_tensor).numpy()\n",
        "        grid[state // 8][state % 8] = np.max(probs)\n",
        "        best_action = np.argmax(probs)\n",
        "        symbols = [\"←\", \"↓\", \"→\", \"↑\"]\n",
        "        action_grid[state // 8][state % 8] = symbols[best_action]\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    sns.heatmap(grid, annot=action_grid, fmt=\"\", cmap=\"coolwarm\", linewidths=0.5)\n",
        "    plt.title(\"Policy Probability Map (VPG)\")\n",
        "    plt.xlabel(\"Column\")\n",
        "    plt.ylabel(\"Row\")\n",
        "    plt.show()\n",
        "\n",
        "def plot_learning_curve(rewards_q, rewards_vpg):\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.plot(rewards_q, label=\"Q-learning\", alpha=0.6)\n",
        "    plt.plot(rewards_vpg, label=\"VPG\", alpha=0.6)\n",
        "    plt.xlabel(\"Episode\")\n",
        "    plt.ylabel(\"Total Reward\")\n",
        "    plt.title(\"Learning Curve Comparison\")\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "plot_q_table_heatmap(q_table, title=\"Q-Learning: Best Actions per State\")\n",
        "\n",
        "plot_policy_probs(policy)\n",
        "\n",
        "plot_learning_curve(rewards_q=q_rewards, rewards_vpg=vpg_rewards)\n"
      ],
      "metadata": {
        "id": "2HORvI7jwFLQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### STEP 6: Animation"
      ],
      "metadata": {
        "id": "IBXP-DBW9sqn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Image, display\n",
        "\n",
        "tile_dict = {\n",
        "    b'S': 'S',\n",
        "    b'F': '.',\n",
        "    b'H': 'H',\n",
        "    b'G': 'G'\n",
        "}\n",
        "agent_icon = 'A'\n",
        "\n",
        "desc = env.unwrapped.desc\n",
        "rows, cols = desc.shape\n",
        "\n",
        "state, _ = env.reset(seed=42)\n",
        "episode_states = [state]\n",
        "done = False\n",
        "while not done:\n",
        "    action = np.argmax(q_table[state])\n",
        "    next_state, reward, terminated, truncated, _ = env.step(action)\n",
        "    episode_states.append(next_state)\n",
        "    state = next_state\n",
        "    done = terminated or truncated\n",
        "\n",
        "def get_rendered_frame(state_idx):\n",
        "    s = episode_states[state_idx]\n",
        "    r, c = divmod(s, cols)\n",
        "    grid = [[tile_dict.get(desc[i, j], '?') for j in range(cols)] for i in range(rows)]\n",
        "    grid[r][c] = agent_icon\n",
        "    return grid\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(cols, rows))\n",
        "plt.axis('off')\n",
        "table = ax.table(cellText=get_rendered_frame(0), loc='center', cellLoc='center')\n",
        "table.scale(1, 1.5)\n",
        "\n",
        "def update(frame_idx):\n",
        "    grid = get_rendered_frame(frame_idx)\n",
        "    for i in range(rows):\n",
        "        for j in range(cols):\n",
        "            table[i, j].get_text().set_text(grid[i][j])\n",
        "    return table\n",
        "\n",
        "ani = animation.FuncAnimation(fig, update, frames=len(episode_states), interval=500, repeat=False)\n",
        "gif_path = \"/content/frozenlake_animation.gif\"\n",
        "ani.save(gif_path, writer='pillow', fps=2)\n",
        "\n",
        "display(Image(filename=gif_path))\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "e1R9YMjF9wDk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}